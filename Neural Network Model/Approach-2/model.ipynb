{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as optim2\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_summary</th>\n",
       "      <th>encoded_tags</th>\n",
       "      <th>Action</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Fantasy</th>\n",
       "      <th>Horror</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>Others</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[tensor(0.0349), tensor(0.0235), tensor(-0.025...</td>\n",
       "      <td>[tensor(0.0476), tensor(-0.0124), tensor(0.024...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[tensor(-0.0059), tensor(-0.0665), tensor(-0.0...</td>\n",
       "      <td>[tensor(0.0418), tensor(-0.0673), tensor(0.039...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[tensor(0.0332), tensor(-0.0196), tensor(-0.00...</td>\n",
       "      <td>[tensor(0.0014), tensor(0.0178), tensor(-0.040...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[tensor(-0.0526), tensor(0.0546), tensor(-0.06...</td>\n",
       "      <td>[tensor(0.0443), tensor(-0.1015), tensor(-0.01...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[tensor(-0.0492), tensor(0.0136), tensor(-0.14...</td>\n",
       "      <td>[tensor(0.0294), tensor(-0.0400), tensor(-0.10...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7733</th>\n",
       "      <td>[tensor(0.0092), tensor(0.0664), tensor(-0.163...</td>\n",
       "      <td>[tensor(0.0636), tensor(-0.0622), tensor(-0.08...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7734</th>\n",
       "      <td>[tensor(0.0011), tensor(0.0388), tensor(-0.100...</td>\n",
       "      <td>[tensor(-0.0060), tensor(-0.0640), tensor(-0.0...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7735</th>\n",
       "      <td>[tensor(-0.0298), tensor(-0.0026), tensor(-0.0...</td>\n",
       "      <td>[tensor(0.0337), tensor(-0.0882), tensor(0.001...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7736</th>\n",
       "      <td>[tensor(-0.0506), tensor(-0.0171), tensor(0.04...</td>\n",
       "      <td>[tensor(0.0048), tensor(-0.0420), tensor(0.012...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7737</th>\n",
       "      <td>[tensor(-0.0218), tensor(0.0245), tensor(-0.05...</td>\n",
       "      <td>[tensor(0.0504), tensor(0.0108), tensor(-0.047...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7738 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        encoded_summary  \\\n",
       "0     [tensor(0.0349), tensor(0.0235), tensor(-0.025...   \n",
       "1     [tensor(-0.0059), tensor(-0.0665), tensor(-0.0...   \n",
       "2     [tensor(0.0332), tensor(-0.0196), tensor(-0.00...   \n",
       "3     [tensor(-0.0526), tensor(0.0546), tensor(-0.06...   \n",
       "4     [tensor(-0.0492), tensor(0.0136), tensor(-0.14...   \n",
       "...                                                 ...   \n",
       "7733  [tensor(0.0092), tensor(0.0664), tensor(-0.163...   \n",
       "7734  [tensor(0.0011), tensor(0.0388), tensor(-0.100...   \n",
       "7735  [tensor(-0.0298), tensor(-0.0026), tensor(-0.0...   \n",
       "7736  [tensor(-0.0506), tensor(-0.0171), tensor(0.04...   \n",
       "7737  [tensor(-0.0218), tensor(0.0245), tensor(-0.05...   \n",
       "\n",
       "                                           encoded_tags  Action  Adventure  \\\n",
       "0     [tensor(0.0476), tensor(-0.0124), tensor(0.024...       0          1   \n",
       "1     [tensor(0.0418), tensor(-0.0673), tensor(0.039...       0          0   \n",
       "2     [tensor(0.0014), tensor(0.0178), tensor(-0.040...       0          0   \n",
       "3     [tensor(0.0443), tensor(-0.1015), tensor(-0.01...       0          0   \n",
       "4     [tensor(0.0294), tensor(-0.0400), tensor(-0.10...       1          0   \n",
       "...                                                 ...     ...        ...   \n",
       "7733  [tensor(0.0636), tensor(-0.0622), tensor(-0.08...       0          0   \n",
       "7734  [tensor(-0.0060), tensor(-0.0640), tensor(-0.0...       0          0   \n",
       "7735  [tensor(0.0337), tensor(-0.0882), tensor(0.001...       0          0   \n",
       "7736  [tensor(0.0048), tensor(-0.0420), tensor(0.012...       0          0   \n",
       "7737  [tensor(0.0504), tensor(0.0108), tensor(-0.047...       0          0   \n",
       "\n",
       "      Animation  Comedy  Crime  Drama  Fantasy  Horror  Musical  Mystery  \\\n",
       "0             0       1      0      1        0       0        0        0   \n",
       "1             0       0      0      1        0       0        0        0   \n",
       "2             0       0      0      1        0       0        0        0   \n",
       "3             0       0      0      0        0       1        0        0   \n",
       "4             0       0      1      0        0       0        0        0   \n",
       "...         ...     ...    ...    ...      ...     ...      ...      ...   \n",
       "7733          0       1      0      1        1       0        0        0   \n",
       "7734          0       0      0      1        0       0        0        1   \n",
       "7735          0       0      0      1        0       0        0        0   \n",
       "7736          0       0      0      0        0       0        0        0   \n",
       "7737          0       0      0      1        0       0        0        0   \n",
       "\n",
       "      Sci-Fi  Thriller  Others  \n",
       "0          0         0       0  \n",
       "1          0         0       0  \n",
       "2          0         0       0  \n",
       "3          0         0       0  \n",
       "4          0         1       0  \n",
       "...      ...       ...     ...  \n",
       "7733       0         0       0  \n",
       "7734       0         1       0  \n",
       "7735       0         0       0  \n",
       "7736       0         0       1  \n",
       "7737       1         1       0  \n",
       "\n",
       "[7738 rows x 15 columns]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = joblib.load(filename='data.joblib')\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of each entry in \"encoded summary\" column is torch.Size([384])\n",
      "Length of each entry in \"encoded tags\" column is torch.Size([384])\n"
     ]
    }
   ],
   "source": [
    "print(f'Length of each entry in \"encoded summary\" column is {data['encoded_summary'][0].shape}')\n",
    "print(f'Length of each entry in \"encoded tags\" column is {data['encoded_tags'][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each entry in the encoded summary column is a 1D tensor of size 384 <br>\n",
    "Each entry in the encoded tag column is a list, that consists of 20 tensors, each of size 384"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_summaries = data['encoded_summary']\n",
    "x=[]\n",
    "for encodings in encoded_summaries:\n",
    "    x.append(encodings)\n",
    "encoded_summaries = torch.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_tags = data['encoded_tags']\n",
    "x=[]\n",
    "for encodings in encoded_tags:\n",
    "    x.append(encodings)\n",
    "encoded_tags = torch.stack(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.drop(['encoded_summary','encoded_tags'],axis=1)\n",
    "y = y.values\n",
    "y = torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of my input : encoded_summary is torch.Size([7738, 384])\n",
      "The size of my input : encoded_tags is torch.Size([7738, 384])\n",
      "The size of my input : y (genre_labels) is torch.Size([7738, 13])\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of my input : encoded_summary is {encoded_summaries.shape}')\n",
    "print(f'The size of my input : encoded_tags is {encoded_tags.shape}')\n",
    "print(f'The size of my input : y (genre_labels) is {y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_summary, X_test_summary, X_train_tags, X_test_tags, y_train, y_test = train_test_split(\n",
    "    encoded_summaries, encoded_tags, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of my input : X_train_summary is torch.Size([6964, 384])\n",
      "The size of my input : X_train_tags is torch.Size([6964, 384])\n",
      "The size of my input : y_train (genre_labels) is torch.Size([6964, 13])\n",
      "The size of my input : X_test_summary is torch.Size([774, 384])\n",
      "The size of my input : X_test_tags is torch.Size([774, 384])\n",
      "The size of my input : y_test (genre_labels) is torch.Size([774, 13])\n"
     ]
    }
   ],
   "source": [
    "print(f'The size of my input : X_train_summary is {X_train_summary.shape}')\n",
    "print(f'The size of my input : X_train_tags is {X_train_tags.shape}')\n",
    "print(f'The size of my input : y_train (genre_labels) is {y_train.shape}')\n",
    "print(f'The size of my input : X_test_summary is {X_test_summary.shape}')\n",
    "print(f'The size of my input : X_test_tags is {X_test_tags.shape}')\n",
    "print(f'The size of my input : y_test (genre_labels) is {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_summary, X_train_tags, y_train)\n",
    "test_dataset = TensorDataset(X_test_summary, X_test_tags, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieDataset(Dataset):\n",
    "    def __init__(self, summaries, tags, labels):\n",
    "        self.summaries = summaries\n",
    "        self.tags = tags\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.summaries)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        summary = self.summaries[idx]\n",
    "        tags = self.tags[idx]\n",
    "        label = self.labels[idx]\n",
    "        return summary, tags, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieGenreClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MovieGenreClassifier, self).__init__()\n",
    "        self.summary_fc = nn.Linear(384, 256)\n",
    "        self.tags_fc = nn.Linear(384, 256)\n",
    "        self.combined_fc1 = nn.Linear(512, 256)\n",
    "        self.combined_fc2 = nn.Linear(256, 128)\n",
    "        self.combined_fc3 = nn.Linear(128, 64)\n",
    "        self.combined_fc4 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 13)\n",
    "\n",
    "    def forward(self, summary=None, tags=None):\n",
    "        if summary is not None:\n",
    "            summary_out = torch.relu(self.summary_fc(summary))\n",
    "        else:\n",
    "            summary_out = torch.zeros((tags.size(0), 256)).to(tags.device)\n",
    "\n",
    "        if tags is not None:\n",
    "            tags_out = torch.relu(self.tags_fc(tags))\n",
    "        else:\n",
    "            tags_out = torch.zeros((summary.size(0), 256)).to(summary.device)\n",
    "        \n",
    "        combined = torch.cat((summary_out, tags_out), dim=1)\n",
    "        combined = torch.relu(self.combined_fc1(combined))\n",
    "        combined = torch.relu(self.combined_fc2(combined))\n",
    "        combined = torch.relu(self.combined_fc3(combined))\n",
    "        combined = torch.relu(self.combined_fc4(combined))\n",
    "        output = torch.sigmoid(self.output(combined))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MovieDataset(encoded_summaries, encoded_tags, y)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MovieGenreClassifier()\n",
    "criterion =  nn.BCEWithLogitsLoss()\n",
    "optimizer = optim2.RangerVA(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Train Loss: 0.6931601892183185\n",
      "Epoch [2/300], Train Loss: 0.6931600917830404\n",
      "Epoch [3/300], Train Loss: 0.6931599997570295\n",
      "Epoch [4/300], Train Loss: 0.6931599024244590\n",
      "Epoch [5/300], Train Loss: 0.6931598138905064\n",
      "Epoch [6/300], Train Loss: 0.6931597186805598\n",
      "Epoch [7/300], Train Loss: 0.6931596323377027\n",
      "Epoch [8/300], Train Loss: 0.6931595410991167\n",
      "Epoch [9/300], Train Loss: 0.6931594507164273\n",
      "Epoch [10/300], Train Loss: 0.6931593751921042\n",
      "Epoch [11/300], Train Loss: 0.6931592878221712\n",
      "Epoch [12/300], Train Loss: 0.6931591979872558\n",
      "Epoch [13/300], Train Loss: 0.6931591191420536\n",
      "Epoch [14/300], Train Loss: 0.6931590353668866\n",
      "Epoch [15/300], Train Loss: 0.6931589544332966\n",
      "Epoch [16/300], Train Loss: 0.6931588752457357\n",
      "Epoch [17/300], Train Loss: 0.6931587955446369\n",
      "Epoch [18/300], Train Loss: 0.6931587232727213\n",
      "Epoch [19/300], Train Loss: 0.6931586432977356\n",
      "Epoch [20/300], Train Loss: 0.6931585649318356\n",
      "Epoch [21/300], Train Loss: 0.6931584876272474\n",
      "Epoch [22/300], Train Loss: 0.6931584162112284\n",
      "Epoch [23/300], Train Loss: 0.6931583409265565\n",
      "Epoch [24/300], Train Loss: 0.6931582739954362\n",
      "Epoch [25/300], Train Loss: 0.6931582004225576\n",
      "Epoch [26/300], Train Loss: 0.6931581254802444\n",
      "Epoch [27/300], Train Loss: 0.6931580585833598\n",
      "Epoch [28/300], Train Loss: 0.6931579844627074\n",
      "Epoch [29/300], Train Loss: 0.6931579249607702\n",
      "Epoch [30/300], Train Loss: 0.6931578556673750\n",
      "Epoch [31/300], Train Loss: 0.6931577895921514\n",
      "Epoch [32/300], Train Loss: 0.6931577243385885\n",
      "Epoch [33/300], Train Loss: 0.6931576534018716\n",
      "Epoch [34/300], Train Loss: 0.6931575884906674\n",
      "Epoch [35/300], Train Loss: 0.6931575277220032\n",
      "Epoch [36/300], Train Loss: 0.6931574650018945\n",
      "Epoch [37/300], Train Loss: 0.6931573979680665\n",
      "Epoch [38/300], Train Loss: 0.6931573390139033\n",
      "Epoch [39/300], Train Loss: 0.6931572756775489\n",
      "Epoch [40/300], Train Loss: 0.6931572142584033\n",
      "Epoch [41/300], Train Loss: 0.6931571576322789\n",
      "Epoch [42/300], Train Loss: 0.6931570998421350\n",
      "Epoch [43/300], Train Loss: 0.6931570339723266\n",
      "Epoch [44/300], Train Loss: 0.6931569784759859\n",
      "Epoch [45/300], Train Loss: 0.6931569244175516\n",
      "Epoch [46/300], Train Loss: 0.6931568611496690\n",
      "Epoch [47/300], Train Loss: 0.6931568098301041\n",
      "Epoch [48/300], Train Loss: 0.6931567526562059\n",
      "Epoch [49/300], Train Loss: 0.6931567016105279\n",
      "Epoch [50/300], Train Loss: 0.6931566439915634\n",
      "Epoch [51/300], Train Loss: 0.6931565903439596\n",
      "Epoch [52/300], Train Loss: 0.6931565370387144\n",
      "Epoch [53/300], Train Loss: 0.6931564853083191\n",
      "Epoch [54/300], Train Loss: 0.6931564310444696\n",
      "Epoch [55/300], Train Loss: 0.6931563822925947\n",
      "Epoch [56/300], Train Loss: 0.6931563295008875\n",
      "Epoch [57/300], Train Loss: 0.6931562770515389\n",
      "Epoch [58/300], Train Loss: 0.6931562286077868\n",
      "Epoch [59/300], Train Loss: 0.6931561742412298\n",
      "Epoch [60/300], Train Loss: 0.6931561295291873\n",
      "Epoch [61/300], Train Loss: 0.6931560795790570\n",
      "Epoch [62/300], Train Loss: 0.6931560332921646\n",
      "Epoch [63/300], Train Loss: 0.6931559791310227\n",
      "Epoch [64/300], Train Loss: 0.6931559358226507\n",
      "Epoch [65/300], Train Loss: 0.6931558864545302\n",
      "Epoch [66/300], Train Loss: 0.6931558391747976\n",
      "Epoch [67/300], Train Loss: 0.6931557893958467\n",
      "Epoch [68/300], Train Loss: 0.6931557524211102\n",
      "Epoch [69/300], Train Loss: 0.6931557077775393\n",
      "Epoch [70/300], Train Loss: 0.6931556633736196\n",
      "Epoch [71/300], Train Loss: 0.6931556143820936\n",
      "Epoch [72/300], Train Loss: 0.6931555780920744\n",
      "Epoch [73/300], Train Loss: 0.6931555308465777\n",
      "Epoch [74/300], Train Loss: 0.6931554867507808\n",
      "Epoch [75/300], Train Loss: 0.6931554470714106\n",
      "Epoch [76/300], Train Loss: 0.6931554002025085\n",
      "Epoch [77/300], Train Loss: 0.6931553613447993\n",
      "Epoch [78/300], Train Loss: 0.6931553202275228\n",
      "Epoch [79/300], Train Loss: 0.6931552785967082\n",
      "Epoch [80/300], Train Loss: 0.6931552411084336\n",
      "Epoch [81/300], Train Loss: 0.6931551948900130\n",
      "Epoch [82/300], Train Loss: 0.6931551566143135\n",
      "Epoch [83/300], Train Loss: 0.6931551198107562\n",
      "Epoch [84/300], Train Loss: 0.6931550804052731\n",
      "Epoch [85/300], Train Loss: 0.6931550430881779\n",
      "Epoch [86/300], Train Loss: 0.6931550004987591\n",
      "Epoch [87/300], Train Loss: 0.6931549596553696\n",
      "Epoch [88/300], Train Loss: 0.6931549275763620\n",
      "Epoch [89/300], Train Loss: 0.6931548893691342\n",
      "Epoch [90/300], Train Loss: 0.6931548505114249\n",
      "Epoch [91/300], Train Loss: 0.6931548115167722\n",
      "Epoch [92/300], Train Loss: 0.6931547753979324\n",
      "Epoch [93/300], Train Loss: 0.6931547417440749\n",
      "Epoch [94/300], Train Loss: 0.6931547059333578\n",
      "Epoch [95/300], Train Loss: 0.6931546645421943\n",
      "Epoch [96/300], Train Loss: 0.6931546342434519\n",
      "Epoch [97/300], Train Loss: 0.6931545948379687\n",
      "Epoch [98/300], Train Loss: 0.6931545604309223\n",
      "Epoch [99/300], Train Loss: 0.6931545293447549\n",
      "Epoch [100/300], Train Loss: 0.6931544988748330\n",
      "Epoch [101/300], Train Loss: 0.6931544588531042\n",
      "Epoch [102/300], Train Loss: 0.6931544248226523\n",
      "Epoch [103/300], Train Loss: 0.6931543929490599\n",
      "Epoch [104/300], Train Loss: 0.6931543625476099\n",
      "Epoch [105/300], Train Loss: 0.6931543256071092\n",
      "Epoch [106/300], Train Loss: 0.6931542990743121\n",
      "Epoch [107/300], Train Loss: 0.6931542595661214\n",
      "Epoch [108/300], Train Loss: 0.6931542281718311\n",
      "Epoch [109/300], Train Loss: 0.6931542005092505\n",
      "Epoch [110/300], Train Loss: 0.6931541666499779\n",
      "Epoch [111/300], Train Loss: 0.6931541391928123\n",
      "Epoch [112/300], Train Loss: 0.6931541043406996\n",
      "Epoch [113/300], Train Loss: 0.6931540718508615\n",
      "Epoch [114/300], Train Loss: 0.6931540409016377\n",
      "Epoch [115/300], Train Loss: 0.6931540106028952\n",
      "Epoch [116/300], Train Loss: 0.6931539876991001\n",
      "Epoch [117/300], Train Loss: 0.6931539496630515\n",
      "Epoch [118/300], Train Loss: 0.6931539216581122\n",
      "Epoch [119/300], Train Loss: 0.6931538958442683\n",
      "Epoch [120/300], Train Loss: 0.6931538684898105\n",
      "Epoch [121/300], Train Loss: 0.6931538353152552\n",
      "Epoch [122/300], Train Loss: 0.6931538074472593\n",
      "Epoch [123/300], Train Loss: 0.6931537774224038\n",
      "Epoch [124/300], Train Loss: 0.6931537559565151\n",
      "Epoch [125/300], Train Loss: 0.6931537260343671\n",
      "Epoch [126/300], Train Loss: 0.6931536938868879\n",
      "Epoch [127/300], Train Loss: 0.6931536702983753\n",
      "Epoch [128/300], Train Loss: 0.6931536417456621\n",
      "Epoch [129/300], Train Loss: 0.6931536150074498\n",
      "Epoch [130/300], Train Loss: 0.6931535874475767\n",
      "Epoch [131/300], Train Loss: 0.6931535604012417\n",
      "Epoch [132/300], Train Loss: 0.6931535371550879\n",
      "Epoch [133/300], Train Loss: 0.6931535107249983\n",
      "Epoch [134/300], Train Loss: 0.6931534776873866\n",
      "Epoch [135/300], Train Loss: 0.6931534558791391\n",
      "Epoch [136/300], Train Loss: 0.6931534273264259\n",
      "Epoch [137/300], Train Loss: 0.6931534054154709\n",
      "Epoch [138/300], Train Loss: 0.6931533794989194\n",
      "Epoch [139/300], Train Loss: 0.6931533575194927\n",
      "Epoch [140/300], Train Loss: 0.6931533238998711\n",
      "Epoch [141/300], Train Loss: 0.6931533022970389\n",
      "Epoch [142/300], Train Loss: 0.6931532767913179\n",
      "Epoch [143/300], Train Loss: 0.6931532510459457\n",
      "Epoch [144/300], Train Loss: 0.6931532266357724\n",
      "Epoch [145/300], Train Loss: 0.6931532039716284\n",
      "Epoch [146/300], Train Loss: 0.6931531823003244\n",
      "Epoch [147/300], Train Loss: 0.6931531531313656\n",
      "Epoch [148/300], Train Loss: 0.6931531358422527\n",
      "Epoch [149/300], Train Loss: 0.6931531111239565\n",
      "Epoch [150/300], Train Loss: 0.6931530888021711\n",
      "Epoch [151/300], Train Loss: 0.6931530616188926\n",
      "Epoch [152/300], Train Loss: 0.6931530432684677\n",
      "Epoch [153/300], Train Loss: 0.6931530138940937\n",
      "Epoch [154/300], Train Loss: 0.6931529976662927\n",
      "Epoch [155/300], Train Loss: 0.6931529725714021\n",
      "Epoch [156/300], Train Loss: 0.6931529517902306\n",
      "Epoch [157/300], Train Loss: 0.6931529221077338\n",
      "Epoch [158/300], Train Loss: 0.6931529025248178\n",
      "Epoch [159/300], Train Loss: 0.6931528813670518\n",
      "Epoch [160/300], Train Loss: 0.6931528631878063\n",
      "Epoch [161/300], Train Loss: 0.6931528407975492\n",
      "Epoch [162/300], Train Loss: 0.6931528144701673\n",
      "Epoch [163/300], Train Loss: 0.6931527996118010\n",
      "Epoch [164/300], Train Loss: 0.6931527747565615\n",
      "Epoch [165/300], Train Loss: 0.6931527572620333\n",
      "Epoch [166/300], Train Loss: 0.6931527311400666\n",
      "Epoch [167/300], Train Loss: 0.6931527173087763\n",
      "Epoch [168/300], Train Loss: 0.6931526954662930\n",
      "Epoch [169/300], Train Loss: 0.6931526760887922\n",
      "Epoch [170/300], Train Loss: 0.6931526560950457\n",
      "Epoch [171/300], Train Loss: 0.6931526333966658\n",
      "Epoch [172/300], Train Loss: 0.6931526117938336\n",
      "Epoch [173/300], Train Loss: 0.6931525902937090\n",
      "Epoch [174/300], Train Loss: 0.6931525761200600\n",
      "Epoch [175/300], Train Loss: 0.6931525526684910\n",
      "Epoch [176/300], Train Loss: 0.6931525372281149\n",
      "Epoch [177/300], Train Loss: 0.6931525170289532\n",
      "Epoch [178/300], Train Loss: 0.6931524930638462\n",
      "Epoch [179/300], Train Loss: 0.6931524817660100\n",
      "Epoch [180/300], Train Loss: 0.6931524562260530\n",
      "Epoch [181/300], Train Loss: 0.6931524383891663\n",
      "Epoch [182/300], Train Loss: 0.6931524225379597\n",
      "Epoch [183/300], Train Loss: 0.6931524007639482\n",
      "Epoch [184/300], Train Loss: 0.6931523917941510\n",
      "Epoch [185/300], Train Loss: 0.6931523662884300\n",
      "Epoch [186/300], Train Loss: 0.6931523445144184\n",
      "Epoch [187/300], Train Loss: 0.6931523289713347\n",
      "Epoch [188/300], Train Loss: 0.6931523097307773\n",
      "Epoch [189/300], Train Loss: 0.6931522914145885\n",
      "Epoch [190/300], Train Loss: 0.6931522763850427\n",
      "Epoch [191/300], Train Loss: 0.6931522608761949\n",
      "Epoch [192/300], Train Loss: 0.6931522421149396\n",
      "Epoch [193/300], Train Loss: 0.6931522226004954\n",
      "Epoch [194/300], Train Loss: 0.6931522060303357\n",
      "Epoch [195/300], Train Loss: 0.6931521914116203\n",
      "Epoch [196/300], Train Loss: 0.6931521759027726\n",
      "Epoch [197/300], Train Loss: 0.6931521535125154\n",
      "Epoch [198/300], Train Loss: 0.6931521377640165\n",
      "Epoch [199/300], Train Loss: 0.6931521282806813\n",
      "Epoch [200/300], Train Loss: 0.6931521059588959\n",
      "Epoch [201/300], Train Loss: 0.6931520901419252\n",
      "Epoch [202/300], Train Loss: 0.6931520747357850\n",
      "Epoch [203/300], Train Loss: 0.6931520571727852\n",
      "Epoch [204/300], Train Loss: 0.6931520377268126\n",
      "Epoch [205/300], Train Loss: 0.6931520267713350\n",
      "Epoch [206/300], Train Loss: 0.6931520070857115\n",
      "Epoch [207/300], Train Loss: 0.6931519945211482\n",
      "Epoch [208/300], Train Loss: 0.6931519784645265\n",
      "Epoch [209/300], Train Loss: 0.6931519575121757\n",
      "Epoch [210/300], Train Loss: 0.6931519463855189\n",
      "Epoch [211/300], Train Loss: 0.6931519328965873\n",
      "Epoch [212/300], Train Loss: 0.6931519160183047\n",
      "Epoch [213/300], Train Loss: 0.6931519003725134\n",
      "Epoch [214/300], Train Loss: 0.6931518856510905\n",
      "Epoch [215/300], Train Loss: 0.6931518695602329\n",
      "Epoch [216/300], Train Loss: 0.6931518523738276\n",
      "Epoch [217/300], Train Loss: 0.6931518332702137\n",
      "Epoch [218/300], Train Loss: 0.6931518150909682\n",
      "Epoch [219/300], Train Loss: 0.6931518077987285\n",
      "Epoch [220/300], Train Loss: 0.6931517899960776\n",
      "Epoch [221/300], Train Loss: 0.6931517770891557\n",
      "Epoch [222/300], Train Loss: 0.6931517642507055\n",
      "Epoch [223/300], Train Loss: 0.6931517492211597\n",
      "Epoch [224/300], Train Loss: 0.6931517305283763\n",
      "Epoch [225/300], Train Loss: 0.6931517197783140\n",
      "Epoch [226/300], Train Loss: 0.6931517083435343\n",
      "Epoch [227/300], Train Loss: 0.6931516921842050\n",
      "Epoch [228/300], Train Loss: 0.6931516773258387\n",
      "Epoch [229/300], Train Loss: 0.6931516642819733\n",
      "Epoch [230/300], Train Loss: 0.6931516471298038\n",
      "Epoch [231/300], Train Loss: 0.6931516427133769\n",
      "Epoch [232/300], Train Loss: 0.6931516222060925\n",
      "Epoch [233/300], Train Loss: 0.6931516066287730\n",
      "Epoch [234/300], Train Loss: 0.6931515956390596\n",
      "Epoch [235/300], Train Loss: 0.6931515814311747\n",
      "Epoch [236/300], Train Loss: 0.6931515661962138\n",
      "Epoch [237/300], Train Loss: 0.6931515547956700\n",
      "Epoch [238/300], Train Loss: 0.6931515453123348\n",
      "Epoch [239/300], Train Loss: 0.6931515264826078\n",
      "Epoch [240/300], Train Loss: 0.6931515196354344\n",
      "Epoch [241/300], Train Loss: 0.6931515012850096\n",
      "Epoch [242/300], Train Loss: 0.6931514868717095\n",
      "Epoch [243/300], Train Loss: 0.6931514766351852\n",
      "Epoch [244/300], Train Loss: 0.6931514613659885\n",
      "Epoch [245/300], Train Loss: 0.6931514467472731\n",
      "Epoch [246/300], Train Loss: 0.6931514309303025\n",
      "Epoch [247/300], Train Loss: 0.6931514256579789\n",
      "Epoch [248/300], Train Loss: 0.6931514088139323\n",
      "Epoch [249/300], Train Loss: 0.6931514045686847\n",
      "Epoch [250/300], Train Loss: 0.6931513833766830\n",
      "Epoch [251/300], Train Loss: 0.6931513734482815\n",
      "Epoch [252/300], Train Loss: 0.6931513565357631\n",
      "Epoch [253/300], Train Loss: 0.6931513491750517\n",
      "Epoch [254/300], Train Loss: 0.6931513341455060\n",
      "Epoch [255/300], Train Loss: 0.6931513235323872\n",
      "Epoch [256/300], Train Loss: 0.6931513075784730\n",
      "Epoch [257/300], Train Loss: 0.6931513059351514\n",
      "Epoch [258/300], Train Loss: 0.6931512868657734\n",
      "Epoch [259/300], Train Loss: 0.6931512723840015\n",
      "Epoch [260/300], Train Loss: 0.6931512578337580\n",
      "Epoch [261/300], Train Loss: 0.6931512522533116\n",
      "Epoch [262/300], Train Loss: 0.6931512435574014\n",
      "Epoch [263/300], Train Loss: 0.6931512297945828\n",
      "Epoch [264/300], Train Loss: 0.6931512118549884\n",
      "Epoch [265/300], Train Loss: 0.6931512033302575\n",
      "Epoch [266/300], Train Loss: 0.6931511912449964\n",
      "Epoch [267/300], Train Loss: 0.6931511843635870\n",
      "Epoch [268/300], Train Loss: 0.6931511707034760\n",
      "Epoch [269/300], Train Loss: 0.6931511577280823\n",
      "Epoch [270/300], Train Loss: 0.6931511489294645\n",
      "Epoch [271/300], Train Loss: 0.6931511384532891\n",
      "Epoch [272/300], Train Loss: 0.6931511298600864\n",
      "Epoch [273/300], Train Loss: 0.6931511132899267\n",
      "Epoch [274/300], Train Loss: 0.6931511018551471\n",
      "Epoch [275/300], Train Loss: 0.6931510915501511\n",
      "Epoch [276/300], Train Loss: 0.6931510866201862\n",
      "Epoch [277/300], Train Loss: 0.6931510744322175\n",
      "Epoch [278/300], Train Loss: 0.6931510606009271\n",
      "Epoch [279/300], Train Loss: 0.6931510514941864\n",
      "Epoch [280/300], Train Loss: 0.6931510349582626\n",
      "Epoch [281/300], Train Loss: 0.6931510253037481\n",
      "Epoch [282/300], Train Loss: 0.6931510143482705\n",
      "Epoch [283/300], Train Loss: 0.6931510054469451\n",
      "Epoch [284/300], Train Loss: 0.6931509927796742\n",
      "Epoch [285/300], Train Loss: 0.6931509894245592\n",
      "Epoch [286/300], Train Loss: 0.6931509747716081\n",
      "Epoch [287/300], Train Loss: 0.6931509646720273\n",
      "Epoch [288/300], Train Loss: 0.6931509532372476\n",
      "Epoch [289/300], Train Loss: 0.6931509470405557\n",
      "Epoch [290/300], Train Loss: 0.6931509361877858\n",
      "Epoch [291/300], Train Loss: 0.6931509268413940\n",
      "Epoch [292/300], Train Loss: 0.6931509101000550\n",
      "Epoch [293/300], Train Loss: 0.6931509027393435\n",
      "Epoch [294/300], Train Loss: 0.6931508864430708\n",
      "Epoch [295/300], Train Loss: 0.6931508859295328\n",
      "Epoch [296/300], Train Loss: 0.6931508722694217\n",
      "Epoch [297/300], Train Loss: 0.6931508625806713\n",
      "Epoch [298/300], Train Loss: 0.6931508554938468\n",
      "Epoch [299/300], Train Loss: 0.6931508453600301\n",
      "Epoch [300/300], Train Loss: 0.6931508348838548\n"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "prev_loss = float('inf')\n",
    "patience = 10 \n",
    "counter = 0\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    train_loss = 0.0\n",
    "    for batch in train_dataloader:\n",
    "        summaries, tags, labels = batch\n",
    "        summaries, tags, labels = summaries.to(device), tags.to(device), labels.to(device)\n",
    "        outputs = model(summary=summaries, tags=tags)\n",
    "        loss = criterion(outputs, labels)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * summaries.size(0)\n",
    "    train_loss = train_loss / len(train_dataloader.dataset)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss:.16f}\")\n",
    "    \n",
    "    # Early stopping criteria\n",
    "    if train_loss >= prev_loss:\n",
    "        counter += 1\n",
    "    else:\n",
    "        counter = 0\n",
    "        prev_loss = train_loss\n",
    "    \n",
    "    if counter >= patience:\n",
    "        print(f'Early stopping after epoch {epoch+1} as loss has not improved.')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.6932\n"
     ]
    }
   ],
   "source": [
    "all_predictions = []\n",
    "true_labels = []\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        summaries, tags, labels = batch\n",
    "        summaries, tags, labels = summaries.to(device), tags.to(device), labels.to(device)\n",
    "        outputs = model(summary=summaries, tags=tags)\n",
    "        loss = criterion(outputs, labels)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        test_loss += loss.item() * summaries.size(0)\n",
    "        all_predictions.append(probabilities.cpu().numpy())\n",
    "        true_labels.append(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "test_loss = test_loss / len(test_dataloader.dataset)\n",
    "all_predictions = np.concatenate(all_predictions, axis=0)\n",
    "true_labels = np.concatenate(true_labels, axis=0)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Predictions:\n",
      "[[0.50000215 0.50000226 0.5000006  0.5000042  0.50000215 0.5000178\n",
      "  0.500002   0.50000423 0.5000013  0.50000185 0.50000244 0.500003\n",
      "  0.5000093 ]\n",
      " [0.5000018  0.5000019  0.5000005  0.5000035  0.5000018  0.5000154\n",
      "  0.50000167 0.5000036  0.50000113 0.50000155 0.500002   0.5000025\n",
      "  0.500008  ]\n",
      " [0.5000018  0.50000197 0.5000005  0.5000036  0.5000018  0.5000157\n",
      "  0.5000017  0.50000364 0.50000113 0.50000155 0.5000021  0.50000256\n",
      "  0.5000081 ]\n",
      " [0.50000185 0.50000197 0.50000054 0.50000364 0.50000185 0.5000159\n",
      "  0.5000017  0.50000376 0.50000113 0.5000016  0.50000215 0.50000256\n",
      "  0.5000082 ]\n",
      " [0.50000197 0.50000215 0.50000054 0.50000393 0.50000197 0.5000169\n",
      "  0.50000185 0.500004   0.50000125 0.5000017  0.50000226 0.5000028\n",
      "  0.5000088 ]\n",
      " [0.5000021  0.50000226 0.5000006  0.5000041  0.5000021  0.50001764\n",
      "  0.50000197 0.50000423 0.5000013  0.5000018  0.5000024  0.5000029\n",
      "  0.5000092 ]\n",
      " [0.50000215 0.50000226 0.5000006  0.5000042  0.50000215 0.5000179\n",
      "  0.500002   0.50000423 0.5000013  0.50000185 0.50000244 0.500003\n",
      "  0.5000093 ]\n",
      " [0.5000021  0.50000226 0.5000006  0.5000041  0.5000021  0.5000177\n",
      "  0.50000197 0.50000423 0.5000013  0.5000018  0.5000024  0.5000029\n",
      "  0.50000924]\n",
      " [0.5000023  0.5000025  0.50000066 0.5000046  0.5000024  0.50001943\n",
      "  0.5000022  0.5000047  0.5000015  0.500002   0.5000027  0.5000033\n",
      "  0.5000102 ]\n",
      " [0.50000226 0.5000024  0.50000066 0.5000044  0.50000226 0.5000187\n",
      "  0.50000215 0.5000045  0.50000143 0.50000197 0.50000256 0.50000316\n",
      "  0.5000098 ]]\n",
      "Example True Labels:\n",
      "[[0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 1. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 3.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Example Predictions:\")\n",
    "print(all_predictions[:10])\n",
    "print(\"Example True Labels:\")\n",
    "print(true_labels[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
