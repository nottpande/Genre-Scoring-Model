{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch_optimizer as optim2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = joblib.load('encodings.joblib')\n",
    "y = joblib.load('genres.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0349,  0.0235, -0.0252,  ...,  0.0743, -0.0681,  0.0263],\n",
       "        [-0.0059, -0.0665, -0.0198,  ...,  0.0336, -0.0681, -0.0098],\n",
       "        [ 0.0332, -0.0196, -0.0088,  ...,  0.0797, -0.0398, -0.0545],\n",
       "        ...,\n",
       "        [-0.0506, -0.0171,  0.0425,  ...,  0.0814, -0.0385, -0.0431],\n",
       "        [-0.0218,  0.0245, -0.0527,  ...,  0.0152,  0.0071,  0.0260],\n",
       "        [ 0.0174,  0.0035, -0.0033,  ...,  0.0411, -0.0065, -0.0919]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = []\n",
    "for tensor in dataset:\n",
    "    x.append(tensor)\n",
    "    \n",
    "X = torch.stack(x)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 1., 0., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y = y.values\n",
    "Y = torch.tensor(Y, dtype=torch.float32) \n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "size of input data = torch.Size([7728, 384])\n",
      "size of output data = torch.Size([7728, 25])\n"
     ]
    }
   ],
   "source": [
    "print(f'size of input data = {X.shape}')\n",
    "print(f'size of output data = {Y.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "train_dataset = GenreDataset(X_train, y_train)\n",
    "test_dataset = GenreDataset(X_test, y_test)\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenreClassifier(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(GenreClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = X.shape[1]\n",
    "output_size = Y.shape[1]\n",
    "model = GenreClassifier(input_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim2.Ranger(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\pytorch_ranger\\ranger.py:172: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1578.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/250], Loss: 109.38300186395645\n",
      "Epoch [2/250], Loss: 42.194820553064346\n",
      "Epoch [3/250], Loss: 41.14140249788761\n",
      "Epoch [4/250], Loss: 39.15633903443813\n",
      "Epoch [5/250], Loss: 36.85499057173729\n",
      "Epoch [6/250], Loss: 34.694492012262344\n",
      "Epoch [7/250], Loss: 32.79312641173601\n",
      "Epoch [8/250], Loss: 31.565377414226532\n",
      "Epoch [9/250], Loss: 30.687852554023266\n",
      "Epoch [10/250], Loss: 29.917442746460438\n",
      "Epoch [11/250], Loss: 29.350926779210567\n",
      "Epoch [12/250], Loss: 28.871544167399406\n",
      "Epoch [13/250], Loss: 28.516992829740047\n",
      "Epoch [14/250], Loss: 28.095064237713814\n",
      "Epoch [15/250], Loss: 27.768005162477493\n",
      "Epoch [16/250], Loss: 27.467013090848923\n",
      "Epoch [17/250], Loss: 27.05176118016243\n",
      "Epoch [18/250], Loss: 26.66102433949709\n",
      "Epoch [19/250], Loss: 26.38297414779663\n",
      "Epoch [20/250], Loss: 25.973081350326538\n",
      "Epoch [21/250], Loss: 25.617038920521736\n",
      "Epoch [22/250], Loss: 25.17306437343359\n",
      "Epoch [23/250], Loss: 24.773290403187275\n",
      "Epoch [24/250], Loss: 24.307392455637455\n",
      "Epoch [25/250], Loss: 23.888618238270283\n",
      "Epoch [26/250], Loss: 23.345577746629715\n",
      "Epoch [27/250], Loss: 22.83701031655073\n",
      "Epoch [28/250], Loss: 22.271085545420647\n",
      "Epoch [29/250], Loss: 21.664280235767365\n",
      "Epoch [30/250], Loss: 21.048490777611732\n",
      "Epoch [31/250], Loss: 20.398105457425117\n",
      "Epoch [32/250], Loss: 19.68387331813574\n",
      "Epoch [33/250], Loss: 19.067594692111015\n",
      "Epoch [34/250], Loss: 18.31663389503956\n",
      "Epoch [35/250], Loss: 17.595093976706266\n",
      "Epoch [36/250], Loss: 16.899804297834635\n",
      "Epoch [37/250], Loss: 16.154972460120916\n",
      "Epoch [38/250], Loss: 15.415658365935087\n",
      "Epoch [39/250], Loss: 14.650547344237566\n",
      "Epoch [40/250], Loss: 13.935924012213945\n",
      "Epoch [41/250], Loss: 13.234407879412174\n",
      "Epoch [42/250], Loss: 12.50005703419447\n",
      "Epoch [43/250], Loss: 11.777786374092102\n",
      "Epoch [44/250], Loss: 11.126867081969976\n",
      "Epoch [45/250], Loss: 10.477995973080397\n",
      "Epoch [46/250], Loss: 9.844753742218018\n",
      "Epoch [47/250], Loss: 9.17944878898561\n",
      "Epoch [48/250], Loss: 8.625283049419522\n",
      "Epoch [49/250], Loss: 7.985028432682157\n",
      "Epoch [50/250], Loss: 7.472575908526778\n",
      "Epoch [51/250], Loss: 6.894856350496411\n",
      "Epoch [52/250], Loss: 6.408189952373505\n",
      "Epoch [53/250], Loss: 5.900719366967678\n",
      "Epoch [54/250], Loss: 5.454762591980398\n",
      "Epoch [55/250], Loss: 4.956355226226151\n",
      "Epoch [56/250], Loss: 4.590628889389336\n",
      "Epoch [57/250], Loss: 4.241040796041489\n",
      "Epoch [58/250], Loss: 3.8854507450014353\n",
      "Epoch [59/250], Loss: 3.5067027984187007\n",
      "Epoch [60/250], Loss: 3.261966015212238\n",
      "Epoch [61/250], Loss: 3.044667821377516\n",
      "Epoch [62/250], Loss: 2.76637906068936\n",
      "Epoch [63/250], Loss: 2.5450378442183137\n",
      "Epoch [64/250], Loss: 2.4072515945881605\n",
      "Epoch [65/250], Loss: 2.181055871769786\n",
      "Epoch [66/250], Loss: 2.0582136823795736\n",
      "Epoch [67/250], Loss: 1.9244021116755903\n",
      "Epoch [68/250], Loss: 1.827757045160979\n",
      "Epoch [69/250], Loss: 1.8085677816998214\n",
      "Epoch [70/250], Loss: 1.6909327390603721\n",
      "Epoch [71/250], Loss: 1.6900681236293167\n",
      "Epoch [72/250], Loss: 1.570344562875107\n",
      "Epoch [73/250], Loss: 1.511713758809492\n",
      "Epoch [74/250], Loss: 1.3805450978688896\n",
      "Epoch [75/250], Loss: 1.3637112469878048\n",
      "Epoch [76/250], Loss: 1.420989107573405\n",
      "Epoch [77/250], Loss: 1.3601288951467723\n",
      "Epoch [78/250], Loss: 1.2940789514686912\n",
      "Epoch [79/250], Loss: 1.340927082928829\n",
      "Epoch [80/250], Loss: 1.3132619214011356\n",
      "Epoch [81/250], Loss: 1.2541680599097162\n",
      "Epoch [82/250], Loss: 1.2815982472384349\n",
      "Epoch [83/250], Loss: 1.5186241792980582\n",
      "Epoch [84/250], Loss: 1.3529758725780994\n",
      "Epoch [85/250], Loss: 1.1940652950434014\n",
      "Epoch [86/250], Loss: 1.2333864611573517\n",
      "Epoch [87/250], Loss: 1.1396548126358539\n",
      "Epoch [88/250], Loss: 1.133994587115012\n",
      "Epoch [89/250], Loss: 1.0419722394435667\n",
      "Epoch [90/250], Loss: 1.1177758489502594\n",
      "Epoch [91/250], Loss: 1.1548938808846287\n",
      "Epoch [92/250], Loss: 1.0691408215789124\n",
      "Epoch [93/250], Loss: 1.0840326186735183\n",
      "Epoch [94/250], Loss: 1.0690103600500152\n",
      "Epoch [95/250], Loss: 1.096405735006556\n",
      "Epoch [96/250], Loss: 1.2798435419099405\n",
      "Epoch [97/250], Loss: 1.138020588667132\n",
      "Epoch [98/250], Loss: 1.0734114180668257\n",
      "Epoch [99/250], Loss: 1.0628425092436373\n",
      "Epoch [100/250], Loss: 1.1427733895252459\n",
      "Epoch [101/250], Loss: 1.1031619592686184\n",
      "Epoch [102/250], Loss: 1.0553027621936053\n",
      "Epoch [103/250], Loss: 1.1045586496475153\n",
      "Epoch [104/250], Loss: 1.0252479464397766\n",
      "Epoch [105/250], Loss: 1.0422809214214794\n",
      "Epoch [106/250], Loss: 1.0061774082714692\n",
      "Epoch [107/250], Loss: 1.1626082418952137\n",
      "Epoch [108/250], Loss: 1.3349724633735605\n",
      "Epoch [109/250], Loss: 1.073290757893119\n",
      "Epoch [110/250], Loss: 1.0243444328079931\n",
      "Epoch [111/250], Loss: 1.0050280536524951\n",
      "Epoch [112/250], Loss: 1.0379251463455148\n",
      "Epoch [113/250], Loss: 1.0758288505021483\n",
      "Epoch [114/250], Loss: 1.0075866423430853\n",
      "Epoch [115/250], Loss: 1.2901377184898593\n",
      "Epoch [116/250], Loss: 1.0251760534592904\n",
      "Epoch [117/250], Loss: 1.1594135080231354\n",
      "Epoch [118/250], Loss: 1.1442783167294692\n",
      "Epoch [119/250], Loss: 1.0890350015833974\n",
      "Epoch [120/250], Loss: 1.0299600920116063\n",
      "Epoch [121/250], Loss: 1.0709657572151627\n",
      "Epoch [122/250], Loss: 1.1517571832519025\n",
      "Epoch [123/250], Loss: 1.0975761625450104\n",
      "Epoch [124/250], Loss: 1.008330351935001\n",
      "Epoch [125/250], Loss: 1.0353252342902124\n",
      "Epoch [126/250], Loss: 1.0814810137962922\n",
      "Epoch [127/250], Loss: 0.9786117023904808\n",
      "Epoch [128/250], Loss: 0.9344977114233188\n",
      "Epoch [129/250], Loss: 1.0345364614913706\n",
      "Epoch [130/250], Loss: 0.9296360503649339\n",
      "Epoch [131/250], Loss: 0.9228829199564643\n",
      "Epoch [132/250], Loss: 1.0424259307328612\n",
      "Epoch [133/250], Loss: 0.9414855666691437\n",
      "Epoch [134/250], Loss: 0.9165242459566798\n",
      "Epoch [135/250], Loss: 0.9530787518015131\n",
      "Epoch [136/250], Loss: 0.9054199485690333\n",
      "Epoch [137/250], Loss: 0.9511521298845764\n",
      "Epoch [138/250], Loss: 0.9478042342525441\n",
      "Epoch [139/250], Loss: 0.8548851772793569\n",
      "Epoch [140/250], Loss: 0.9693572043324821\n",
      "Epoch [141/250], Loss: 0.9902914820704609\n",
      "Epoch [142/250], Loss: 0.9569794536801055\n",
      "Epoch [143/250], Loss: 1.0307406418723986\n",
      "Epoch [144/250], Loss: 0.9127408626663964\n",
      "Epoch [145/250], Loss: 1.017064421408577\n",
      "Epoch [146/250], Loss: 0.8651167300122324\n",
      "Epoch [147/250], Loss: 0.8820985803322401\n",
      "Epoch [148/250], Loss: 0.9253019740281161\n",
      "Epoch [149/250], Loss: 0.9654646872077137\n",
      "Epoch [150/250], Loss: 0.9582003372488543\n",
      "Epoch [151/250], Loss: 1.1331744764465839\n",
      "Epoch [152/250], Loss: 1.0203442239144351\n",
      "Epoch [153/250], Loss: 1.0449220597511157\n",
      "Epoch [154/250], Loss: 0.8803453575237654\n",
      "Epoch [155/250], Loss: 0.9458508238894865\n",
      "Epoch [156/250], Loss: 0.9130065971112344\n",
      "Epoch [157/250], Loss: 0.8690210454515181\n",
      "Epoch [158/250], Loss: 0.9639058702450711\n",
      "Epoch [159/250], Loss: 0.9441996917594224\n",
      "Epoch [160/250], Loss: 0.8771583315101452\n",
      "Epoch [161/250], Loss: 0.9017147212580312\n",
      "Epoch [162/250], Loss: 1.026672689447878\n",
      "Epoch [163/250], Loss: 0.9022255381860305\n",
      "Epoch [164/250], Loss: 0.95529235349386\n",
      "Epoch [165/250], Loss: 0.9026230229937937\n",
      "Epoch [166/250], Loss: 0.8991972315707244\n",
      "Epoch [167/250], Loss: 0.7799253033008426\n",
      "Epoch [168/250], Loss: 0.8651340294163674\n",
      "Epoch [169/250], Loss: 0.8597303635033313\n",
      "Epoch [170/250], Loss: 0.8937507190858014\n",
      "Epoch [171/250], Loss: 1.0299866678833496\n",
      "Epoch [172/250], Loss: 0.9406888607190922\n",
      "Epoch [173/250], Loss: 1.097253141226247\n",
      "Epoch [174/250], Loss: 0.9661475903121755\n",
      "Epoch [175/250], Loss: 0.9869776376581285\n",
      "Epoch [176/250], Loss: 0.9524258999590529\n",
      "Epoch [177/250], Loss: 0.9886024334118702\n",
      "Epoch [178/250], Loss: 0.9398309440002777\n",
      "Epoch [179/250], Loss: 0.9301516258856282\n",
      "Epoch [180/250], Loss: 1.0266793800983578\n",
      "Epoch [181/250], Loss: 1.1136358392250258\n",
      "Epoch [182/250], Loss: 1.066474156890763\n",
      "Epoch [183/250], Loss: 1.0136833611468319\n",
      "Epoch [184/250], Loss: 1.064492464211071\n",
      "Epoch [185/250], Loss: 1.1093476728710812\n",
      "Epoch [186/250], Loss: 1.001937732740771\n",
      "Epoch [187/250], Loss: 1.1968658456462435\n",
      "Epoch [188/250], Loss: 1.0130869808781426\n",
      "Epoch [189/250], Loss: 1.113094331289176\n",
      "Epoch [190/250], Loss: 1.0464652305090567\n",
      "Epoch [191/250], Loss: 1.0522556437936146\n",
      "Epoch [192/250], Loss: 1.04118436286808\n",
      "Epoch [193/250], Loss: 1.0541477322985884\n",
      "Epoch [194/250], Loss: 1.035573923640186\n",
      "Epoch [195/250], Loss: 0.9631814519234467\n",
      "Epoch [196/250], Loss: 0.9036010034615174\n",
      "Epoch [197/250], Loss: 0.9476479669392575\n",
      "Epoch [198/250], Loss: 0.9229098866635468\n",
      "Epoch [199/250], Loss: 0.8944978989602532\n",
      "Epoch [200/250], Loss: 0.8859938390960451\n",
      "Epoch [201/250], Loss: 0.882161702378653\n",
      "Epoch [202/250], Loss: 0.9349904701521154\n",
      "Epoch [203/250], Loss: 0.9165581960405689\n",
      "Epoch [204/250], Loss: 0.9539913548796903\n",
      "Epoch [205/250], Loss: 0.924666765233269\n",
      "Epoch [206/250], Loss: 0.8335066540166736\n",
      "Epoch [207/250], Loss: 0.9147087114397436\n",
      "Epoch [208/250], Loss: 0.9238024922087789\n",
      "Epoch [209/250], Loss: 0.8811713539762422\n",
      "Epoch [210/250], Loss: 0.8518959189241286\n",
      "Epoch [211/250], Loss: 0.9146953586314339\n",
      "Epoch [212/250], Loss: 0.95708383913734\n",
      "Epoch [213/250], Loss: 0.8829973467072705\n",
      "Epoch [214/250], Loss: 0.9318583483982366\n",
      "Epoch [215/250], Loss: 0.7631529403442983\n",
      "Epoch [216/250], Loss: 0.7139241026015952\n",
      "Epoch [217/250], Loss: 0.7100609636690933\n",
      "Epoch [218/250], Loss: 0.7333285118802451\n",
      "Epoch [219/250], Loss: 0.7521966289205011\n",
      "Epoch [220/250], Loss: 0.8862635696132202\n",
      "Epoch [221/250], Loss: 0.9142846484028269\n",
      "Epoch [222/250], Loss: 0.8816934705100721\n",
      "Epoch [223/250], Loss: 0.7994261484127492\n",
      "Epoch [224/250], Loss: 0.7654566805576906\n",
      "Epoch [225/250], Loss: 0.8134648582781665\n",
      "Epoch [226/250], Loss: 0.8776499099622015\n",
      "Epoch [227/250], Loss: 0.8599593049730174\n",
      "Epoch [228/250], Loss: 0.8151314544084016\n",
      "Epoch [229/250], Loss: 0.8376082002214389\n",
      "Epoch [230/250], Loss: 0.8226706128916703\n",
      "Epoch [231/250], Loss: 0.8628465091169346\n",
      "Epoch [232/250], Loss: 0.8926947370928247\n",
      "Epoch [233/250], Loss: 0.893589288694784\n",
      "Epoch [234/250], Loss: 0.8421963228320237\n",
      "Epoch [235/250], Loss: 0.7747199256700696\n",
      "Epoch [236/250], Loss: 0.8376655921456404\n",
      "Epoch [237/250], Loss: 0.8678315808647312\n",
      "Epoch [238/250], Loss: 0.8188320881308755\n",
      "Epoch [239/250], Loss: 0.8560996422893368\n",
      "Epoch [240/250], Loss: 0.7799534169898834\n",
      "Epoch [241/250], Loss: 0.77773977315519\n",
      "Epoch [242/250], Loss: 0.8179153648379724\n",
      "Epoch [243/250], Loss: 0.8235505422926508\n",
      "Epoch [244/250], Loss: 0.827302545963903\n",
      "Epoch [245/250], Loss: 0.8092914490407566\n",
      "Epoch [246/250], Loss: 0.8591384566680063\n",
      "Epoch [247/250], Loss: 0.8100645619269926\n",
      "Epoch [248/250], Loss: 0.8075745914975414\n",
      "Epoch [249/250], Loss: 0.8241337695071707\n",
      "Epoch [250/250], Loss: 0.8446079336863477\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "genre_scores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        genre_score = probabilities * 10\n",
    "        genre_scores.append(genre_score.numpy())\n",
    "\n",
    "# Concatenate all batch results\n",
    "genre_score_array = np.concatenate(genre_scores, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_labels = y.columns.tolist()\n",
    "score_df = pd.DataFrame(genre_score_array, columns=genre_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Action</th>\n",
       "      <th>Adult</th>\n",
       "      <th>Adventure</th>\n",
       "      <th>Animation</th>\n",
       "      <th>Biography</th>\n",
       "      <th>Comedy</th>\n",
       "      <th>Crime</th>\n",
       "      <th>Documentary</th>\n",
       "      <th>Drama</th>\n",
       "      <th>Family</th>\n",
       "      <th>...</th>\n",
       "      <th>Musical</th>\n",
       "      <th>Mystery</th>\n",
       "      <th>News</th>\n",
       "      <th>None</th>\n",
       "      <th>Sci-Fi</th>\n",
       "      <th>Short</th>\n",
       "      <th>Sport</th>\n",
       "      <th>Thriller</th>\n",
       "      <th>War</th>\n",
       "      <th>Western</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.194819</td>\n",
       "      <td>7.306678</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.023396</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.159925</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000029</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.219317</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000017</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.309942</td>\n",
       "      <td>5.000007</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000214</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000296</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000002</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.974951</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.212441</td>\n",
       "      <td>5.000628</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>6.922373</td>\n",
       "      <td>5.000671</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000012</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000699</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.002545</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000351</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.228055</td>\n",
       "      <td>5.003295</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.233229</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000012</td>\n",
       "      <td>5.000031</td>\n",
       "      <td>5.663621</td>\n",
       "      <td>5.001967</td>\n",
       "      <td>5.279696</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.052836</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000002</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000004</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.952491</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1541</th>\n",
       "      <td>5.052267</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.002139</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000033</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.287879</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.142103</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000001</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.007449</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1542</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000029</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.882365</td>\n",
       "      <td>7.302532</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000021</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1543</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.588245</td>\n",
       "      <td>5.221994</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000217</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1544</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000048</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.020325</td>\n",
       "      <td>7.310586</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000003</td>\n",
       "      <td>5.000100</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1545</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000010</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>7.310485</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000005</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000064</td>\n",
       "      <td>5.000012</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.229548</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1546 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Action     Adult  Adventure  Animation  Biography    Comedy     Crime  \\\n",
       "0     5.000000  5.000000   5.194819   7.306678   5.000000  5.023396  5.000000   \n",
       "1     5.000000  5.000017   5.000000   5.000000   5.000000  7.309942  5.000007   \n",
       "2     5.000000  5.000000   5.000000   5.000000   5.000000  7.212441  5.000628   \n",
       "3     5.000000  5.000000   5.000000   5.000000   5.000000  5.000000  5.000000   \n",
       "4     5.000000  5.000000   5.000000   5.000000   5.000012  5.000031  5.663621   \n",
       "...        ...       ...        ...        ...        ...       ...       ...   \n",
       "1541  5.052267  5.000000   5.002139   5.000000   5.000000  5.000000  5.000033   \n",
       "1542  5.000000  5.000000   5.000000   5.000000   5.000000  5.000029  5.000000   \n",
       "1543  5.000000  5.000000   5.000000   5.000000   5.000000  5.000000  5.000000   \n",
       "1544  5.000000  5.000000   5.000048   5.000000   5.000000  5.000000  5.020325   \n",
       "1545  5.000000  5.000000   5.000000   5.000010   5.000000  5.000000  5.000000   \n",
       "\n",
       "      Documentary     Drama    Family  ...   Musical   Mystery      News  \\\n",
       "0        5.000000  5.000000  7.159925  ...  5.000029  5.000000  5.000000   \n",
       "1        5.000000  5.000214  5.000000  ...  5.000000  5.000296  5.000000   \n",
       "2        5.000000  6.922373  5.000671  ...  5.000000  5.000012  5.000000   \n",
       "3        5.000000  5.002545  5.000000  ...  5.000000  5.000351  5.000000   \n",
       "4        5.001967  5.279696  5.000000  ...  5.000000  5.052836  5.000000   \n",
       "...           ...       ...       ...  ...       ...       ...       ...   \n",
       "1541     5.000000  7.287879  5.000000  ...  5.000000  7.142103  5.000000   \n",
       "1542     5.000000  5.882365  7.302532  ...  5.000000  5.000000  5.000000   \n",
       "1543     5.588245  5.221994  5.000000  ...  5.000000  5.000000  5.000000   \n",
       "1544     7.310586  5.000000  5.000000  ...  5.000000  5.000003  5.000100   \n",
       "1545     7.310485  5.000000  5.000000  ...  5.000000  5.000000  5.000005   \n",
       "\n",
       "          None    Sci-Fi     Short  Sport  Thriller       War   Western  \n",
       "0     5.000000  5.000000  7.219317    5.0  5.000000  5.000000  5.000002  \n",
       "1     5.000000  5.000002  5.000000    5.0  6.974951  5.000000  5.000000  \n",
       "2     5.000000  5.000000  5.000000    5.0  5.000699  5.000000  5.000000  \n",
       "3     5.000000  7.228055  5.003295    5.0  6.233229  5.000000  5.000000  \n",
       "4     5.000002  5.000000  5.000004    5.0  6.952491  5.000000  5.000000  \n",
       "...        ...       ...       ...    ...       ...       ...       ...  \n",
       "1541  5.000000  5.000001  5.000000    5.0  5.007449  5.000000  5.000000  \n",
       "1542  5.000000  5.000000  5.000021    5.0  5.000000  5.000000  5.000000  \n",
       "1543  5.000000  5.000000  5.000000    5.0  5.000000  5.000217  5.000000  \n",
       "1544  5.000000  5.000000  5.000000    5.0  5.000000  5.000000  5.000000  \n",
       "1545  5.000000  5.000064  5.000012    5.0  5.229548  5.000000  5.000000  \n",
       "\n",
       "[1546 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully.\n"
     ]
    }
   ],
   "source": [
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, 'scoring_model_1.pth')\n",
    "\n",
    "print(\"Model saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
